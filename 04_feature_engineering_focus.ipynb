{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experiment 2: Feature Engineering and Dimensionality Reduction**\n",
    "\n",
    "## ***Objectives for this Notebook***\n",
    "\n",
    "Machine learning models thrive on good data. But raw data often comes messy and unrefined, holding hidden gems amidst irrelevant clutter. This is where feature engineering and dimensionality reduction come in like superheroes, transforming your data from unpolished ore to gleaming treasure.\n",
    "\n",
    "Feature engineering and dimensionality reduction are two crucial steps in the machine learning pipeline that can significantly improve the performance of your models.\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into features that are suitable for training and deploying machine learning models. Simply, it's about:\n",
    "* Selecting the right features: Choosing the most relevant features that hold predictive power for your target variable. Think carefully, irrelevant features can mislead your model!\n",
    "* Creating new features: Combining existing features or extracting hidden patterns to unlock deeper insights. New features can be like secret weapons for your model!\n",
    "* Transforming features: Scaling, normalizing, or encoding categorical data to ensure all features play fair in the model's eyes. No one wants features dominating the competition due to unfair advantages!\n",
    "\n",
    "\n",
    "**Dimensionality reduction** is a technique used in machine learning and statistics to reduce the number of features or variables in a dataset while preserving its essential information. The goal is to simplify the data and improve computational efficiency, mitigate the curse of dimensionality, and enhance the performance of machine learning models.\n",
    "\n",
    "Algorithms:\n",
    "* Non-Negative Matrix Factorization (NMF): NMF is a factorization technique that decomposes a matrix into two non-negative matrices. It is particularly useful for non-negative data, such as images or text, and is often applied in topic modeling and image processing.\n",
    "* Latent Dirichlet Allocation (LDA): LDA is a probabilistic generative model used for topic modeling. It assumes that documents are mixtures of topics and that each word's presence is attributable to one of the document's topics. LDA helps discover the underlying topics in a collection of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.) Loading the dataset and setting parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, PCA\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "\n",
    "import random\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "seed_value = 42\n",
    "# Set Python seed\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set NumPy seed\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set scikit-learn seed\n",
    "sklearn_random_state = check_random_state(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Simonsiella</th>\n",
       "      <th>Treponema</th>\n",
       "      <th>Campylobacter</th>\n",
       "      <th>Helicobacter</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Comamonas</th>\n",
       "      <th>Pseudomonas</th>\n",
       "      <th>Xanthomonas</th>\n",
       "      <th>Agrobacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Hungatella</th>\n",
       "      <th>Pseudopropionibacterium</th>\n",
       "      <th>Peptoanaerobacter</th>\n",
       "      <th>Emergencia</th>\n",
       "      <th>Prevotellamassilia</th>\n",
       "      <th>Criibacterium</th>\n",
       "      <th>Fournierella</th>\n",
       "      <th>Negativibacillus</th>\n",
       "      <th>Duodenibacillus</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-CG-5720-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-CN-4741-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HNSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-BR-6801-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-IG-A3I8-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ESCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-L5-A4OT-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ESCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>TCGA-CG-5719-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>TCGA-CQ-5329-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HNSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>TCGA-CQ-7068-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HNSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>TCGA-CG-4455-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>TCGA-AG-A020-01A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>READ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  Simonsiella  Treponema  Campylobacter  Helicobacter  \\\n",
       "0    TCGA-CG-5720-01A          0.0   0.000000       0.000000      0.895050   \n",
       "1    TCGA-CN-4741-01A          0.0   0.000000       0.010470      0.000000   \n",
       "2    TCGA-BR-6801-01A          0.0   0.000000       0.000000      0.000000   \n",
       "3    TCGA-IG-A3I8-01A          0.0   0.000000       0.000000      0.067717   \n",
       "4    TCGA-L5-A4OT-01A          0.0   0.000000       0.012202      0.000000   \n",
       "..                ...          ...        ...            ...           ...   \n",
       "507  TCGA-CG-5719-01A          0.0   0.000000       0.000000      0.106557   \n",
       "508  TCGA-CQ-5329-01A          0.0   0.175564       0.000000      0.000000   \n",
       "509  TCGA-CQ-7068-01A          0.0   0.335060       0.000000      0.000000   \n",
       "510  TCGA-CG-4455-01A          0.0   0.000000       0.000000      0.000000   \n",
       "511  TCGA-AG-A020-01A          0.0   0.000000       0.000000      0.000000   \n",
       "\n",
       "     Paracoccus  Comamonas  Pseudomonas  Xanthomonas  Agrobacterium  ...  \\\n",
       "0           0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "1           0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "2           0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "3           0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "4           0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "..          ...        ...          ...          ...            ...  ...   \n",
       "507         0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "508         0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "509         0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "510         0.0        0.0     0.014781          0.0            0.0  ...   \n",
       "511         0.0        0.0     0.000000          0.0            0.0  ...   \n",
       "\n",
       "     Hungatella  Pseudopropionibacterium  Peptoanaerobacter  Emergencia  \\\n",
       "0           0.0                      0.0           0.000000         0.0   \n",
       "1           0.0                      0.0           0.000000         0.0   \n",
       "2           0.0                      0.0           0.000000         0.0   \n",
       "3           0.0                      0.0           0.000000         0.0   \n",
       "4           0.0                      0.0           0.000000         0.0   \n",
       "..          ...                      ...                ...         ...   \n",
       "507         0.0                      0.0           0.000000         0.0   \n",
       "508         0.0                      0.0           0.136613         0.0   \n",
       "509         0.0                      0.0           0.011534         0.0   \n",
       "510         0.0                      0.0           0.000000         0.0   \n",
       "511         0.0                      0.0           0.000000         0.0   \n",
       "\n",
       "     Prevotellamassilia  Criibacterium  Fournierella  Negativibacillus  \\\n",
       "0                   0.0            0.0           0.0               0.0   \n",
       "1                   0.0            0.0           0.0               0.0   \n",
       "2                   0.0            0.0           0.0               0.0   \n",
       "3                   0.0            0.0           0.0               0.0   \n",
       "4                   0.0            0.0           0.0               0.0   \n",
       "..                  ...            ...           ...               ...   \n",
       "507                 0.0            0.0           0.0               0.0   \n",
       "508                 0.0            0.0           0.0               0.0   \n",
       "509                 0.0            0.0           0.0               0.0   \n",
       "510                 0.0            0.0           0.0               0.0   \n",
       "511                 0.0            0.0           0.0               0.0   \n",
       "\n",
       "     Duodenibacillus  label  \n",
       "0                0.0   STAD  \n",
       "1                0.0   HNSC  \n",
       "2                0.0   STAD  \n",
       "3                0.0   ESCA  \n",
       "4                0.0   ESCA  \n",
       "..               ...    ...  \n",
       "507              0.0   STAD  \n",
       "508              0.0   HNSC  \n",
       "509              0.0   HNSC  \n",
       "510              0.0   STAD  \n",
       "511              0.0   READ  \n",
       "\n",
       "[512 rows x 133 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "microbiome_df = pd.read_csv(\"./dataset/microbiome_preprocessed_files/microbiome_merged_dfs.csv\")\n",
    "microbiome_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"HNSC\", \"STAD\", \"COAD\", \"ESCA\", \"READ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.) Creating a data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_validation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.785829</td>\n",
       "      <td>0.911599</td>\n",
       "      <td>0.831183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAD</td>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.780853</td>\n",
       "      <td>0.981524</td>\n",
       "      <td>0.793080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COAD</td>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.895644</td>\n",
       "      <td>0.965040</td>\n",
       "      <td>0.892741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESCA</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596405</td>\n",
       "      <td>0.734715</td>\n",
       "      <td>0.669170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>READ</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.676505</td>\n",
       "      <td>0.609679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label criterion  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "0  HNSC   entropy         20                 1                 40   \n",
       "1  STAD   entropy         30                 1                 10   \n",
       "2  COAD      gini         20                 1                 10   \n",
       "3  ESCA      gini         10                 1                 10   \n",
       "4  READ   entropy         10                 1                 20   \n",
       "\n",
       "   n_estimators  test_score  mean_train_score  mean_validation_score  \n",
       "0            50    0.785829          0.911599               0.831183  \n",
       "1           100    0.780853          0.981524               0.793080  \n",
       "2           200    0.895644          0.965040               0.892741  \n",
       "3             1    0.596405          0.734715               0.669170  \n",
       "4             1    0.700000          0.676505               0.609679  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1_best_hyperparam_report = pd.read_csv(\"./dataset/microbiome_preprocessed_files/exp1_best_hyperparam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create hyperparameters, we use the best hyperparameters for the rf classifier from experiment 1\n",
    "\n",
    "def create_hyperparameter_grids(report_df, additional_param=None):\n",
    "    new_hyperparamter_grid_by_class = {}\n",
    "\n",
    "    # Read the best hyperparameters from experiment 1\n",
    "    exclude_cols = [\"label\", \"test_score\", \"mean_train_score\", \"mean_validation_score\"] # We don't want these columns to be in our future grid parameters\n",
    "    if report_df is None or report_df.empty:\n",
    "        raise ValueError(\"Report DataFrame is required.\")\n",
    "\n",
    "    for c in report_df.index: \n",
    "        # Choose a row (class) from the report DataFrame\n",
    "        selected_class = report_df.index[c]\n",
    "        # Get the best hyperparameters and remove excluded columns\n",
    "        exp1_best_hyperparameters = report_df.loc[selected_class].drop(exclude_cols).to_dict()\n",
    "\n",
    "        exp1_best_hyperparameters = {f\"{key}\": [value] for key, value in exp1_best_hyperparameters.items()} # The format we want for classifier hyperparameters\n",
    "\n",
    "        if additional_param != None:\n",
    "            # Add the additional parameters\n",
    "            exp1_best_hyperparameters = {f\"rf__{key}\": value for key, value in exp1_best_hyperparameters.items()} #Adjust prefix of hyperparameters\n",
    "            combined_param_grid = {**exp1_best_hyperparameters, **additional_param}\n",
    "            new_hyperparamter_grid_by_class[report_df.loc[selected_class][\"label\"]] = combined_param_grid\n",
    "        else:\n",
    "            new_hyperparamter_grid_by_class[report_df.loc[selected_class][\"label\"]] = exp1_best_hyperparameters\n",
    "\n",
    "    return new_hyperparamter_grid_by_class\n",
    "\n",
    "nmf_pca_param_grid = {\n",
    "    \"transformer__n_components\": list(range(4,65)) # In paper, features 4 to 64\n",
    "}\n",
    "\n",
    "lda_param_grid = {\n",
    "    \"transformer__n_components\": list(range(1, len(classes))) # In paper, features 1 to length of class-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(X,y,hyperparameters,algorithm=\"NMF\"):\n",
    "\n",
    "    augmented_datasets_dict = {}\n",
    "    \n",
    "    # Create the pipeline object. Here we are chaining the dimensionality reduction step and the classifier step.\n",
    "\n",
    "    algorithms_dict = {\"NMF\": NMF(), \"LDA\": LatentDirichletAllocation(), \"PCA\": PCA()}\n",
    "    pipeline = Pipeline([\n",
    "        ('transformer', algorithms_dict[algorithm]),\n",
    "        ('rf', RandomForestClassifier(random_state=seed_value))\n",
    "    ])\n",
    "\n",
    "    # hyperparameters\n",
    "    combined_param_grid = create_hyperparameter_grids(hyperparameters,lda_param_grid if algorithm==\"LDA\" else nmf_pca_param_grid)\n",
    " \n",
    "    print(f'Using algorithm {algorithm}:')\n",
    "    for i in classes:\n",
    "        print(f\"Class {i}\")\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed_value)\n",
    "\n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=combined_param_grid[i],\n",
    "            scoring=make_scorer(balanced_accuracy_score),  # Choose an appropriate metric for your problem\n",
    "            cv=cv,\n",
    "            n_jobs=-1,  # Use all available processors\n",
    "            return_train_score=True  # Set to True to calculate train scores\n",
    "        )\n",
    "\n",
    "        # Perform the grid search\n",
    "        print(\"Fitting..\")\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Print the best hyperparameters\n",
    "        print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # LENGTH OF DATASET SHOULD STAY SIMILAR TO ORIGINAL, but length of features should increase by the best n_components\n",
    "\n",
    "        # Transform the original dataset to get the latent features\n",
    "        latent_features = best_model.named_steps['transformer'].transform(X)\n",
    "        print(f\"Count of new columns/latent features: {len(latent_features.T)}\")\n",
    "\n",
    "        print(f\"Count of columns of original feature set: {len(X.T)}\")\n",
    "\n",
    "        feature_augmented = pd.concat((X, pd.DataFrame(latent_features, index=X.index, columns=[f\"new_feature {str(x)}\" for x in range(1,len(latent_features.T)+1)])), axis=1)\n",
    "        print(f\"Count of columns of augmented feature set: {len(feature_augmented.T)}\")\n",
    "        print(f\"Count of rows of augmented dataset: {len(feature_augmented)}\")\n",
    "\n",
    "        augmented_datasets_dict[i] = feature_augmented\n",
    "\n",
    "    return augmented_datasets_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_2_data_loader(dataframe, \n",
    "                         label_column, \n",
    "                         classes, \n",
    "                         report_df=exp1_best_hyperparam_report, \n",
    "                         algorithm=\"NMF\", \n",
    "                         train_test=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "    - dataset_dict: a dictionary where the keys is the targeted class and the values are its corresponding features and labels\n",
    "    \"\"\"\n",
    "    augmented_dataset_dict = {}\n",
    "\n",
    "    exclude_cols = [\"name\", \"label\"]\n",
    "\n",
    "    feat = dataframe.drop(columns=exclude_cols)\n",
    "    tar = dataframe[\"label\"]\n",
    "\n",
    "    # print(f\"len of feat: {len(feat)}\")\n",
    "    # print(f\"len of tar: {len(tar)}\")\n",
    "\n",
    "    augmented_features_dict = augment_dataset(feat, tar,hyperparameters=report_df, algorithm=algorithm)\n",
    "\n",
    "\n",
    "    # print(f\"Count of rows in original dataset:{len(dataframe)}\")\n",
    "    # print(f\"Count of rows in augmented dataset:{len(feature_augmented)}\")\n",
    "\n",
    "    for i in classes:\n",
    "        positive_class = i\n",
    "        dframe = pd.DataFrame(index=augmented_features_dict[i].index)\n",
    "        dframe[\"name\"] = dataframe[\"name\"]\n",
    "        dframe = pd.concat([dframe, augmented_features_dict[i]], axis=1)\n",
    "        dframe['label'] = [1 if x == positive_class else 0 for x in dataframe[label_column]]\n",
    "        print(dframe.label.value_counts())\n",
    "        X = dframe.drop([\"name\", \"label\"], axis=1)\n",
    "        y = dframe[\"label\"]\n",
    "        if train_test:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=seed_value)\n",
    "            augmented_dataset_dict[positive_class] = {\"train\": (X_train, y_train),\n",
    "                                            \"test\": (X_test, y_test)}\n",
    "        else:\n",
    "            augmented_dataset_dict[positive_class] = {\"feature\": X, \n",
    "                                            \"label\": y}\n",
    "\n",
    "    return augmented_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algorithm NMF:\n",
      "Class HNSC\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 40, 'rf__n_estimators': 50, 'transformer__n_components': 15}\n",
      "Count of new columns/latent features: 15\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 146\n",
      "Count of rows of augmented dataset: 512\n",
      "Class STAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 100, 'transformer__n_components': 27}\n",
      "Count of new columns/latent features: 27\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 158\n",
      "Count of rows of augmented dataset: 512\n",
      "Class COAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 200, 'transformer__n_components': 26}\n",
      "Count of new columns/latent features: 26\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 157\n",
      "Count of rows of augmented dataset: 512\n",
      "Class ESCA\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 1, 'transformer__n_components': 53}\n",
      "Count of new columns/latent features: 53\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 184\n",
      "Count of rows of augmented dataset: 512\n",
      "Class READ\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 20, 'rf__n_estimators': 1, 'transformer__n_components': 26}\n",
      "Count of new columns/latent features: 26\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 157\n",
      "Count of rows of augmented dataset: 512\n",
      "label\n",
      "0    357\n",
      "1    155\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    385\n",
      "1    127\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    387\n",
      "1    125\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    452\n",
      "1     60\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    467\n",
      "1     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "exp2_datasets_nmf = exp_2_data_loader(microbiome_df, \"label\", classes=classes, report_df=exp1_best_hyperparam_report, algorithm=\"NMF\", train_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algorithm LDA:\n",
      "Class HNSC\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 40, 'rf__n_estimators': 50, 'transformer__n_components': 3}\n",
      "Count of new columns/latent features: 3\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 134\n",
      "Count of rows of augmented dataset: 512\n",
      "Class STAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 100, 'transformer__n_components': 4}\n",
      "Count of new columns/latent features: 4\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 135\n",
      "Count of rows of augmented dataset: 512\n",
      "Class COAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 200, 'transformer__n_components': 4}\n",
      "Count of new columns/latent features: 4\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 135\n",
      "Count of rows of augmented dataset: 512\n",
      "Class ESCA\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 1, 'transformer__n_components': 4}\n",
      "Count of new columns/latent features: 4\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 135\n",
      "Count of rows of augmented dataset: 512\n",
      "Class READ\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 20, 'rf__n_estimators': 1, 'transformer__n_components': 3}\n",
      "Count of new columns/latent features: 3\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 134\n",
      "Count of rows of augmented dataset: 512\n",
      "label\n",
      "0    357\n",
      "1    155\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    385\n",
      "1    127\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    387\n",
      "1    125\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    452\n",
      "1     60\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    467\n",
      "1     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "exp2_datasets_lda = exp_2_data_loader(microbiome_df, \"label\", classes=classes, report_df=exp1_best_hyperparam_report, algorithm=\"LDA\", train_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algorithm PCA:\n",
      "Class HNSC\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 40, 'rf__n_estimators': 50, 'transformer__n_components': 43}\n",
      "Count of new columns/latent features: 43\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 174\n",
      "Count of rows of augmented dataset: 512\n",
      "Class STAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 100, 'transformer__n_components': 44}\n",
      "Count of new columns/latent features: 44\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 175\n",
      "Count of rows of augmented dataset: 512\n",
      "Class COAD\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 20, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 200, 'transformer__n_components': 64}\n",
      "Count of new columns/latent features: 64\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 195\n",
      "Count of rows of augmented dataset: 512\n",
      "Class ESCA\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'gini', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 1, 'transformer__n_components': 48}\n",
      "Count of new columns/latent features: 48\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 179\n",
      "Count of rows of augmented dataset: 512\n",
      "Class READ\n",
      "Fitting..\n",
      "Best Hyperparameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 20, 'rf__n_estimators': 1, 'transformer__n_components': 51}\n",
      "Count of new columns/latent features: 51\n",
      "Count of columns of original feature set: 131\n",
      "Count of columns of augmented feature set: 182\n",
      "Count of rows of augmented dataset: 512\n",
      "label\n",
      "0    357\n",
      "1    155\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    385\n",
      "1    127\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    387\n",
      "1    125\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    452\n",
      "1     60\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    467\n",
      "1     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "exp2_datasets_pca = exp_2_data_loader(microbiome_df, \"label\", classes=classes, report_df=exp1_best_hyperparam_report, algorithm=\"PCA\", train_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simonsiella</th>\n",
       "      <th>Treponema</th>\n",
       "      <th>Campylobacter</th>\n",
       "      <th>Helicobacter</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Comamonas</th>\n",
       "      <th>Pseudomonas</th>\n",
       "      <th>Xanthomonas</th>\n",
       "      <th>Agrobacterium</th>\n",
       "      <th>Bradyrhizobium</th>\n",
       "      <th>...</th>\n",
       "      <th>new_feature 6</th>\n",
       "      <th>new_feature 7</th>\n",
       "      <th>new_feature 8</th>\n",
       "      <th>new_feature 9</th>\n",
       "      <th>new_feature 10</th>\n",
       "      <th>new_feature 11</th>\n",
       "      <th>new_feature 12</th>\n",
       "      <th>new_feature 13</th>\n",
       "      <th>new_feature 14</th>\n",
       "      <th>new_feature 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.019054</td>\n",
       "      <td>0.032869</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.021345</td>\n",
       "      <td>0.017670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072503</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.028632</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048651</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284333</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>0.059244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035827</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029941</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows Ã— 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Simonsiella  Treponema  Campylobacter  Helicobacter  Paracoccus  \\\n",
       "155          0.0   0.010944            0.0      0.000000         0.0   \n",
       "414          0.0   0.000000            0.0      0.107591         0.0   \n",
       "172          0.0   0.000000            0.0      0.000000         0.0   \n",
       "367          0.0   0.000000            0.0      0.421787         0.0   \n",
       "462          0.0   0.000000            0.0      0.000000         0.0   \n",
       "..           ...        ...            ...           ...         ...   \n",
       "33           0.0   0.000000            0.0      0.000000         0.0   \n",
       "15           0.0   0.000000            0.0      0.000000         0.0   \n",
       "198          0.0   0.000000            0.0      0.000000         0.0   \n",
       "211          0.0   0.000000            0.0      0.000000         0.0   \n",
       "494          0.0   0.000000            0.0      0.000000         0.0   \n",
       "\n",
       "     Comamonas  Pseudomonas  Xanthomonas  Agrobacterium  Bradyrhizobium  ...  \\\n",
       "155        0.0     0.017489          0.0            0.0             0.0  ...   \n",
       "414        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "172        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "367        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "462        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "..         ...          ...          ...            ...             ...  ...   \n",
       "33         0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "15         0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "198        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "211        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "494        0.0     0.000000          0.0            0.0             0.0  ...   \n",
       "\n",
       "     new_feature 6  new_feature 7  new_feature 8  new_feature 9  \\\n",
       "155       0.000091       0.019054       0.032869       0.006719   \n",
       "414       0.072503       0.017535       0.028632       0.001330   \n",
       "172       0.000000       0.000000       0.000000       0.000000   \n",
       "367       0.284333       0.020627       0.059244       0.000000   \n",
       "462       0.000000       0.000000       0.000325       0.000000   \n",
       "..             ...            ...            ...            ...   \n",
       "33        0.000000       0.017618       0.000873       0.000000   \n",
       "15        0.000312       0.000199       0.000822       0.000000   \n",
       "198       0.000000       0.029941       0.000236       0.000000   \n",
       "211       0.000000       0.000000       0.000144       0.000000   \n",
       "494       0.000000       0.086458       0.000000       0.000000   \n",
       "\n",
       "     new_feature 10  new_feature 11  new_feature 12  new_feature 13  \\\n",
       "155        0.000503        0.000111        0.001004        0.002378   \n",
       "414        0.000000        0.000024        0.000130        0.000000   \n",
       "172        0.000000        0.000921        0.000000        0.002354   \n",
       "367        0.000000        0.000000        0.002250        0.000000   \n",
       "462        0.000000        0.000179        0.000106        0.000000   \n",
       "..              ...             ...             ...             ...   \n",
       "33         0.000574        0.000129        0.000138        0.000000   \n",
       "15         0.004203        0.000000        0.000775        0.014377   \n",
       "198        0.000000        0.000000        0.000000        0.000000   \n",
       "211        0.000000        0.010166        0.000055        0.000139   \n",
       "494        0.056205        0.000000        0.000000        0.000000   \n",
       "\n",
       "     new_feature 14  new_feature 15  \n",
       "155        0.021345        0.017670  \n",
       "414        0.048651        0.000000  \n",
       "172        0.000000        0.000996  \n",
       "367        0.035827        0.000000  \n",
       "462        0.000000        0.000830  \n",
       "..              ...             ...  \n",
       "33         0.000000        0.000064  \n",
       "15         0.000000        0.000000  \n",
       "198        0.011949        0.000499  \n",
       "211        0.000000        0.000423  \n",
       "494        0.000000        0.000000  \n",
       "\n",
       "[435 rows x 146 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2_datasets_nmf[\"HNSC\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_validation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.785829</td>\n",
       "      <td>0.911599</td>\n",
       "      <td>0.831183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAD</td>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.780853</td>\n",
       "      <td>0.981524</td>\n",
       "      <td>0.793080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COAD</td>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.895644</td>\n",
       "      <td>0.965040</td>\n",
       "      <td>0.892741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESCA</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596405</td>\n",
       "      <td>0.734715</td>\n",
       "      <td>0.669170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>READ</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.676505</td>\n",
       "      <td>0.609679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label criterion  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "0  HNSC   entropy         20                 1                 40   \n",
       "1  STAD   entropy         30                 1                 10   \n",
       "2  COAD      gini         20                 1                 10   \n",
       "3  ESCA      gini         10                 1                 10   \n",
       "4  READ   entropy         10                 1                 20   \n",
       "\n",
       "   n_estimators  test_score  mean_train_score  mean_validation_score  \n",
       "0            50    0.785829          0.911599               0.831183  \n",
       "1           100    0.780853          0.981524               0.793080  \n",
       "2           200    0.895644          0.965040               0.892741  \n",
       "3             1    0.596405          0.734715               0.669170  \n",
       "4             1    0.700000          0.676505               0.609679  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1_best_hyperparam_by_class = create_hyperparameter_grids(exp1_best_hyperparam_report, None)\n",
    "exp1_best_hyperparam_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gridsearchcv(dataset_dict, classes, cv_n_splits=5, n_jobs=12):\n",
    "    report = {}\n",
    "    for c in classes:\n",
    "        print(\"Class: \", c)\n",
    "        features = dataset_dict[c][\"train\"][0]\n",
    "        target = dataset_dict[c][\"train\"][1]\n",
    "\n",
    "        # Define the classifier\n",
    "        rf = RandomForestClassifier(random_state=seed_value)\n",
    "\n",
    "        exp1_best_hyperparameter_grids = create_hyperparameter_grids(exp1_best_hyperparam_report, None)\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=cv_n_splits, shuffle=True, random_state=seed_value)\n",
    "\n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=exp1_best_hyperparameter_grids[c],\n",
    "            scoring=make_scorer(balanced_accuracy_score),  # Choose an appropriate metric for your problem\n",
    "            cv=cv,\n",
    "            n_jobs=-1,  # Use all available processors,\n",
    "            return_train_score=True\n",
    ")\n",
    "\n",
    "        # Perform the grid search\n",
    "        grid_search.fit(features, target)\n",
    "\n",
    "        # Access cv_results_ attribute to get detailed results\n",
    "        cv_results = grid_search.cv_results_\n",
    "\n",
    "        # Print the best hyperparameters\n",
    "        print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluate the best model on your test set\n",
    "        # Assuming you have X_test and y_test from your data\n",
    "        # Modify this based on your actual test set\n",
    "        predictions = best_model.predict(dataset_dict[c][\"test\"][0])\n",
    "        accuracy = balanced_accuracy_score(dataset_dict[c][\"test\"][1], predictions)\n",
    "        print(\"Accuracy on Test Set:\", accuracy)\n",
    "\n",
    "\n",
    "        # Calculate mean train score and mean test score\n",
    "        mean_train_score = cv_results['mean_train_score'][grid_search.best_index_]\n",
    "        mean_test_score = cv_results['mean_test_score'][grid_search.best_index_]\n",
    "\n",
    "        print(\"Mean Train Score:\", mean_train_score)\n",
    "        print(\"Mean Test Score:\", mean_test_score)\n",
    "\n",
    "\n",
    "        report[c] = {\n",
    "            \"model\": best_model,\n",
    "            \"best_hyperparameters\": grid_search.best_params_,\n",
    "            \"test_score\": accuracy,\n",
    "            \"mean_train_score\": mean_train_score,\n",
    "            \"mean_validation_score\": mean_test_score\n",
    "        }\n",
    "        \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  HNSC\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 50}\n",
      "Accuracy on Test Set: 0.7455716586151369\n",
      "Mean Train Score: 0.8906344127746835\n",
      "Mean Test Score: 0.7867037971136333\n",
      "Class:  STAD\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Accuracy on Test Set: 0.780852994555354\n",
      "Mean Train Score: 0.9718924060663319\n",
      "Mean Test Score: 0.7768231768231768\n",
      "Class:  COAD\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Accuracy on Test Set: 0.8870235934664247\n",
      "Mean Train Score: 0.9740275595201787\n",
      "Mean Test Score: 0.8702530802530802\n",
      "Class:  ESCA\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.5890522875816994\n",
      "Mean Train Score: 0.7551205454194649\n",
      "Mean Test Score: 0.6635064935064935\n",
      "Class:  READ\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.5571428571428572\n",
      "Mean Train Score: 0.670942538622573\n",
      "Mean Test Score: 0.5896948462929477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HNSC': {'model': RandomForestClassifier(criterion='entropy', max_depth=20, min_samples_split=40,\n",
       "                         n_estimators=50, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 40,\n",
       "   'n_estimators': 50},\n",
       "  'test_score': 0.7455716586151369,\n",
       "  'mean_train_score': 0.8906344127746835,\n",
       "  'mean_validation_score': 0.7867037971136333},\n",
       " 'STAD': {'model': RandomForestClassifier(criterion='entropy', max_depth=30, min_samples_split=10,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 100},\n",
       "  'test_score': 0.780852994555354,\n",
       "  'mean_train_score': 0.9718924060663319,\n",
       "  'mean_validation_score': 0.7768231768231768},\n",
       " 'COAD': {'model': RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=200,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 200},\n",
       "  'test_score': 0.8870235934664247,\n",
       "  'mean_train_score': 0.9740275595201787,\n",
       "  'mean_validation_score': 0.8702530802530802},\n",
       " 'ESCA': {'model': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=1,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.5890522875816994,\n",
       "  'mean_train_score': 0.7551205454194649,\n",
       "  'mean_validation_score': 0.6635064935064935},\n",
       " 'READ': {'model': RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=20,\n",
       "                         n_estimators=1, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 20,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.5571428571428572,\n",
       "  'mean_train_score': 0.670942538622573,\n",
       "  'mean_validation_score': 0.5896948462929477}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2_nmf_report = perform_gridsearchcv(exp2_datasets_nmf, classes)\n",
    "exp2_nmf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_hyperparameter_to_df(nested_dict, exp_name):\n",
    "    path = \"./dataset/microbiome_preprocessed_files/\"\n",
    "    data = []\n",
    "\n",
    "    for key, value in nested_dict.items():\n",
    "        entry = {'label': key}\n",
    "        entry.update(value['best_hyperparameters'])\n",
    "        entry['test_score'] = value['test_score']\n",
    "        entry['mean_train_score'] = value['mean_train_score']\n",
    "        entry['mean_validation_score'] = value['mean_validation_score']\n",
    "        data.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df.to_csv(path+f\"{exp_name}_best_hyperparam.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_validation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.745572</td>\n",
       "      <td>0.890634</td>\n",
       "      <td>0.786704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAD</td>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.780853</td>\n",
       "      <td>0.971892</td>\n",
       "      <td>0.776823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COAD</td>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.887024</td>\n",
       "      <td>0.974028</td>\n",
       "      <td>0.870253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESCA</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.518791</td>\n",
       "      <td>0.739436</td>\n",
       "      <td>0.632980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>READ</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.670943</td>\n",
       "      <td>0.589695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label criterion  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "0  HNSC   entropy         20                 1                 40   \n",
       "1  STAD   entropy         30                 1                 10   \n",
       "2  COAD      gini         20                 1                 10   \n",
       "3  ESCA      gini         10                 1                 10   \n",
       "4  READ   entropy         10                 1                 20   \n",
       "\n",
       "   n_estimators  test_score  mean_train_score  mean_validation_score  \n",
       "0            50    0.745572          0.890634               0.786704  \n",
       "1           100    0.780853          0.971892               0.776823  \n",
       "2           200    0.887024          0.974028               0.870253  \n",
       "3             1    0.518791          0.739436               0.632980  \n",
       "4             1    0.557143          0.670943               0.589695  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameter_to_df(exp2_nmf_report, \"exp2_nmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  HNSC\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 50}\n",
      "Accuracy on Test Set: 0.7548309178743962\n",
      "Mean Train Score: 0.8978200475170173\n",
      "Mean Test Score: 0.8194435103451496\n",
      "Class:  STAD\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Accuracy on Test Set: 0.7722323049001816\n",
      "Mean Train Score: 0.9711127571311096\n",
      "Mean Test Score: 0.785001665001665\n",
      "Class:  COAD\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Accuracy on Test Set: 0.8784029038112522\n",
      "Mean Train Score: 0.9736985634212207\n",
      "Mean Test Score: 0.8914418914418913\n",
      "Class:  ESCA\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.6299019607843137\n",
      "Mean Train Score: 0.711817057178144\n",
      "Mean Test Score: 0.5935338345864661\n",
      "Class:  READ\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.5571428571428572\n",
      "Mean Train Score: 0.6407173408136868\n",
      "Mean Test Score: 0.5529249547920434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HNSC': {'model': RandomForestClassifier(criterion='entropy', max_depth=20, min_samples_split=40,\n",
       "                         n_estimators=50, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 40,\n",
       "   'n_estimators': 50},\n",
       "  'test_score': 0.7548309178743962,\n",
       "  'mean_train_score': 0.8978200475170173,\n",
       "  'mean_validation_score': 0.8194435103451496},\n",
       " 'STAD': {'model': RandomForestClassifier(criterion='entropy', max_depth=30, min_samples_split=10,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 100},\n",
       "  'test_score': 0.7722323049001816,\n",
       "  'mean_train_score': 0.9711127571311096,\n",
       "  'mean_validation_score': 0.785001665001665},\n",
       " 'COAD': {'model': RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=200,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 200},\n",
       "  'test_score': 0.8784029038112522,\n",
       "  'mean_train_score': 0.9736985634212207,\n",
       "  'mean_validation_score': 0.8914418914418913},\n",
       " 'ESCA': {'model': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=1,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.6299019607843137,\n",
       "  'mean_train_score': 0.711817057178144,\n",
       "  'mean_validation_score': 0.5935338345864661},\n",
       " 'READ': {'model': RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=20,\n",
       "                         n_estimators=1, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 20,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.5571428571428572,\n",
       "  'mean_train_score': 0.6407173408136868,\n",
       "  'mean_validation_score': 0.5529249547920434}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2_lda_report = perform_gridsearchcv(exp2_datasets_lda, classes)\n",
    "exp2_lda_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_validation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSC</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.754831</td>\n",
       "      <td>0.897820</td>\n",
       "      <td>0.819444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAD</td>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.772232</td>\n",
       "      <td>0.971113</td>\n",
       "      <td>0.785002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COAD</td>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.878403</td>\n",
       "      <td>0.973699</td>\n",
       "      <td>0.891442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESCA</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629902</td>\n",
       "      <td>0.711817</td>\n",
       "      <td>0.593534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>READ</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.640717</td>\n",
       "      <td>0.552925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label criterion  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "0  HNSC   entropy         20                 1                 40   \n",
       "1  STAD   entropy         30                 1                 10   \n",
       "2  COAD      gini         20                 1                 10   \n",
       "3  ESCA      gini         10                 1                 10   \n",
       "4  READ   entropy         10                 1                 20   \n",
       "\n",
       "   n_estimators  test_score  mean_train_score  mean_validation_score  \n",
       "0            50    0.754831          0.897820               0.819444  \n",
       "1           100    0.772232          0.971113               0.785002  \n",
       "2           200    0.878403          0.973699               0.891442  \n",
       "3             1    0.629902          0.711817               0.593534  \n",
       "4             1    0.557143          0.640717               0.552925  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameter_to_df(exp2_lda_report, \"exp2_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  HNSC\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 50}\n",
      "Accuracy on Test Set: 0.7673107890499196\n",
      "Mean Train Score: 0.9136923226841276\n",
      "Mean Test Score: 0.7605202933071786\n",
      "Class:  STAD\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Accuracy on Test Set: 0.7282214156079855\n",
      "Mean Train Score: 0.9869165896370118\n",
      "Mean Test Score: 0.7227805527805528\n",
      "Class:  COAD\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Accuracy on Test Set: 0.8607078039927405\n",
      "Mean Train Score: 0.976613263348893\n",
      "Mean Test Score: 0.8694105894105896\n",
      "Class:  ESCA\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.4893790849673203\n",
      "Mean Train Score: 0.7855913929956593\n",
      "Mean Test Score: 0.5661654135338345\n",
      "Class:  READ\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 1}\n",
      "Accuracy on Test Set: 0.5357142857142857\n",
      "Mean Train Score: 0.7159559498826555\n",
      "Mean Test Score: 0.5921157323688969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HNSC': {'model': RandomForestClassifier(criterion='entropy', max_depth=20, min_samples_split=40,\n",
       "                         n_estimators=50, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 40,\n",
       "   'n_estimators': 50},\n",
       "  'test_score': 0.7673107890499196,\n",
       "  'mean_train_score': 0.9136923226841276,\n",
       "  'mean_validation_score': 0.7605202933071786},\n",
       " 'STAD': {'model': RandomForestClassifier(criterion='entropy', max_depth=30, min_samples_split=10,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 100},\n",
       "  'test_score': 0.7282214156079855,\n",
       "  'mean_train_score': 0.9869165896370118,\n",
       "  'mean_validation_score': 0.7227805527805528},\n",
       " 'COAD': {'model': RandomForestClassifier(max_depth=20, min_samples_split=10, n_estimators=200,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 200},\n",
       "  'test_score': 0.8607078039927405,\n",
       "  'mean_train_score': 0.976613263348893,\n",
       "  'mean_validation_score': 0.8694105894105896},\n",
       " 'ESCA': {'model': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=1,\n",
       "                         random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'gini',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.4893790849673203,\n",
       "  'mean_train_score': 0.7855913929956593,\n",
       "  'mean_validation_score': 0.5661654135338345},\n",
       " 'READ': {'model': RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=20,\n",
       "                         n_estimators=1, random_state=42),\n",
       "  'best_hyperparameters': {'criterion': 'entropy',\n",
       "   'max_depth': 10,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 20,\n",
       "   'n_estimators': 1},\n",
       "  'test_score': 0.5357142857142857,\n",
       "  'mean_train_score': 0.7159559498826555,\n",
       "  'mean_validation_score': 0.5921157323688969}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2_pca_report = perform_gridsearchcv(exp2_datasets_pca, classes)\n",
    "exp2_pca_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
